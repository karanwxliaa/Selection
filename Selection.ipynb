{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Starter Code\n",
        "##This is for basic dataloading and to give a basic structure\n",
        "### Obviously this is the VERY BASIC structure of the code with minimal code that probably doesnt make sense, this is for you to get started with and have an indea what to do\n",
        "\n",
        "### Write your own code and formulate your own approach, feel free to delete this, **i suggest NOT using it**. Also try using Pytorch if possible"
      ],
      "metadata": {
        "id": "qEiJV0mJk3YO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-29KdCUhZVK",
        "outputId": "c804f6c1-a801-41f0-d04a-2ba31df505bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "WjfiE842hZ26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/selection_competition/train.zip'\n",
        "extract_path = '/content/'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# Step 3: Preprocess the images\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import pandas as pd\n",
        "\n",
        "# Load Data\n",
        "data_dir = '/content/drive/MyDrive/selection_competition/'\n",
        "labels_csv = os.path.join(data_dir, 'train_labels.csv')\n",
        "labels_df = pd.read_csv(labels_csv)\n",
        "\n",
        "\n",
        "# Set the directory where images are extracted\n",
        "data_dir = extract_path\n",
        "\n",
        "# Preprocess Data\n",
        "def preprocess_image(image_path):\n",
        "    try:\n",
        "        image = Image.open(image_path)\n",
        "        image = image.resize((128, 128))\n",
        "        image = np.array(image) / 255.0\n",
        "        return image\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Create Dataset\n",
        "images = []\n",
        "labels = []\n",
        "for i, row in labels_df.iterrows():\n",
        "    if i >= 500:\n",
        "        break\n",
        "    image_path = os.path.join(data_dir+\"train/\", row['id'] + '.tif')\n",
        "    if os.path.exists(image_path):\n",
        "        image = preprocess_image(image_path)\n",
        "        if image is not None:\n",
        "            images.append(image)\n",
        "            labels.append(row['label'])\n",
        "    else:\n",
        "        #print(f\"Image {image_path} does not exist.\")\n",
        "        pass\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Debug: Print the shape of the loaded images and labels\n",
        "print(f\"Loaded {len(images)} images and {len(labels)} labels.\")\n",
        "print(f\"Image shape: {images.shape}\")\n",
        "print(f\"Labels shape: {labels.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gczrfe_9iPTI",
        "outputId": "95b6769d-502f-4551-ee97-84575f134cbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 500 images and 500 labels.\n",
            "Image shape: (500, 128, 128, 3)\n",
            "Labels shape: (500,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Augmentation\n",
        "datagen = ImageDataGenerator(rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True)\n",
        "datagen.fit(images)\n",
        "\n",
        "# Split Data\n",
        "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "fljFKo0Ti19a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Build Model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train Model\n",
        "model.fit(datagen.flow(X_train, y_train, batch_size=32), validation_data=(X_val, y_val), epochs=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOM7nVsuh0we",
        "outputId": "e20a121d-4368-42a3-e40a-386d487295f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "13/13 [==============================] - 14s 889ms/step - loss: 2.3026 - accuracy: 0.4825 - val_loss: 0.6683 - val_accuracy: 0.6000\n",
            "Epoch 2/2\n",
            "13/13 [==============================] - 12s 885ms/step - loss: 0.7077 - accuracy: 0.5825 - val_loss: 0.5520 - val_accuracy: 0.8200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d948ec3fe20>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Reshape, Dense, Flatten, Dropout, Activation\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization, LeakyReLU, UpSampling2D, Conv2D\n",
        "\n",
        "# Build Generator\n",
        "def build_generator():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128 * 32 * 32, activation=\"relu\", input_dim=100))\n",
        "    model.add(Reshape((32, 32, 128)))\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Conv2D(3, kernel_size=3, padding=\"same\"))\n",
        "    model.add(Activation(\"tanh\"))\n",
        "    return model\n",
        "\n",
        "# Build Discriminator\n",
        "def build_discriminator():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=(128, 128, 3), padding=\"same\"))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Compile GAN\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "gan_input = Input(shape=(100,))\n",
        "generated_image = generator(gan_input)\n",
        "discriminator.trainable = False\n",
        "gan_output = discriminator(generated_image)\n",
        "\n",
        "gan = Model(gan_input, gan_output)\n",
        "gan.compile(loss='binary_crossentropy', optimizer='adam')\n"
      ],
      "metadata": {
        "id": "ekQTj1vdj3QV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch_geometric\n"
      ],
      "metadata": {
        "id": "iB9d104Ekocw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# Example data preparation for GCN\n",
        "edge_index = torch.tensor([[0, 1, 1, 2], [1, 0, 2, 1]], dtype=torch.long)  # Example edge index\n",
        "x = torch.tensor([[1], [1], [1]], dtype=torch.float)  # Example node features\n",
        "\n",
        "# Define GCN\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(1, 16)\n",
        "        self.conv2 = GCNConv(16, 2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return nn.LogSoftmax(dim=1)(x)\n",
        "\n",
        "# Prepare data for GCN\n",
        "data = Data(x=x, edge_index=edge_index)\n",
        "\n",
        "# Train GCN\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GCN().to(device)\n",
        "data = data.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model.train()\n",
        "for epoch in range(200):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = criterion(out, torch.tensor([0, 1, 0], dtype=torch.long).to(device))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_ojorI0kSnv",
        "outputId": "3ad82896-7ef6-4a95-be28-8d2d2f079b8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Training\n"
          ]
        }
      ]
    }
  ]
}
